[{"content":" @Article{kerbl3Dgaussians, author = {Kerbl, Bernhard and Kopanas, Georgios and Leimkhler, Thomas and Drettakis, George}, title = {3D Gaussian Splatting for Real-Time Radiance Field Rendering}, journal = {ACM Transactions on Graphics}, number = {4}, volume = {42}, month = {July}, year = {2023}, url = {https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/} } Project\nCode\n高解像度(1080p)で学習時間が短く、描画時間もリアルタイム（\u0026gt;=30fps）であるようなレンダリング手法を提案する。\nカメラキャリブレーションから生成されるスパースな点から3Dガウシアンを使ってシーンを表現する。この3Dガウシアンはシーンを最適化する連続的なボリューム輝度場を持ちながら、空のスペースに対しては不要な計算を行わないという理想的な性質を持つ。 3Dガウシアンのインタリーブ最適化と密度制御を行い、異方性共分散を最適化することでシーンを正確に表現する。 Visibility-Awareな高速レンダリングを行う。異方性スプラフティングをサポートし、学習の高速化＆リアルタイムレンダリングを可能にする。 比較 NeRFやその派生系(voxel, hash grids, points)のVolumetric Ray-Marchingを使った方法は連続値を扱えるので最適化しやすいが、サンプリングに時間がかかったり、ノイジーになりやすいデメリットがある。\nこの論文のゴール\n複数のシーン画像からのリアルタイムレンダリング 既存手法よりも数倍速い最適化 3D Gaussian Splatting 全体図 SfMのスパースな点群を出発点として、高解像度な新規視点のシーンを最適化する必要があり、それには微分可能なボリュームレンダリングの性質を持ちつつ、高速なレンダリングができるプリミティブが必要。 3D Gaussianはそういった性質を持つし、アルファブレンディングできる2D上に投影することができる。\n3D Gaussianに近い論文＝小さな法線をもつ円盤を使う\nしかし、これは法線を推定するのが困難で、それを最適化するのも難しい。\n3D Gaussianは世界座標系で中心点（平均） $\\mu$ の3次元共分散行列 $\\Sigma$ で定義される。\n$$ G(x)=e^{-\\frac{1}{2}(x)^T \\Sigma^{-1}(x)} $$\nこのガウシアンは$\\alpha$を掛けることでブレンディングできる。\n3Dから2Dにレンダリングするときは、次式で行える。\n$$ \\Sigma^\\prime = JW\\Sigma W^TJ^T $$\nここで、J＝透視投影変換のアフィン近似のヤコビアン、W＝視点変換行列、$\\Sigma^\\prime$＝カメラ座標系の共分散行列。\nこの3D Gaussianの3次元共分散行列 $\\Sigma$を直接最適化できれば良いが、共分散行列は半正定値のときのみ物理的な意味を持つのでうまく行かない。制約を設けて勾配降下法をしてもうまく行かない。 純粋な共分散行列を最適化しようとしてもだめなので、3D Gaussianを楕円体とみて共分散行列っぽいものを作ってみる。\n$$ \\Sigma = RSS^TR^T $$\nここで、R＝回転行列、S＝スケール行列。このRとSを両方最適化したいがうまくいかないので、別々に最適化する。 （自動微分のオーバーヘッドを避けるために、明示的に手計算で微分している模様\u0026hellip;）\n最適化後の3Dガウシアン（右）。異方性のある形状を表現できている。 3D Gaussianの適応的密度制御による最適化 最適化 3D Gaussianの\n密度\n+ 位置\n+ 透明度 $\\alpha$\n+ 共分散行列 $\\Sigma$\n+ 球面調和係数（各Gaussianの色）\nの最適化を行う。\n反復的なレンダリングで生成された画像と学習画像とを比較して最適化を行う。しかし、3D-\u0026gt;2Dの投影は曖昧さを含み不確実性が伴う。 そのため、まず幾何情報を作成して、間違っていたら削除したり動かしたりできるような最適化を行う。\nGPUアクセラレートフレームワークやカスタムCUDAカーネルを追加して高速化を図っている。後述の高速なラスタライズも重要。\n$\\alpha$に対してはsigmoid関数を適用して[0, 1)になるようにしたり、勾配がなめらかになるようにしている。また、指数関数的な活性化関数は共分散行列のスケールするのに使っている。\n初期の共分散行列は近傍の3点への距離平均に等しい軸を持つ等方性のガウシアン。 Plenoxelsと同様に指数的減衰スケジューラを使用するが、「位置」のみに適用。 損失関数としては次式になる。これを確率的勾配降下法(SGD)で最適化。\n$$ \\mathcal{L}=(1-\\lambda) \\mathcal{L}_1+\\lambda \\mathcal{L}_S $$\nここで、$\\lambda=0.2$\n適応的なガウシアンの制御 SfMのスパースな点群を初期位置として、適応的にガウシアンの数や単位ボリューム（ガウシアンではなく、その場所の密度という意味だと思う）の密度を制御する。\nWarm-up最適化をしたあと、100イテレーションごとに密にして、$\\alpha$がしきい値$\\epsilon_{\\alpha}$を下回ったら削除する。\nガウシアンの適応的な制御は空のスペースに配置する必要がある。 これは、幾何学的な特徴が欠落している領域に焦点を当てるだけではなく、ガウシアンがカバーするシーンの大きな領域の両方に焦点を当てている。（意味がよくわからない）\n$\\tau_{pos}=0.0002$以上の位置勾配の大きさをもつガウシアンを高密度化。（上図）小さいガウシアンはクローンして位置勾配の方向に従って移動。（下図）大きな分散を持つ大きいガウシアンは小さく分割。 最適化をしていくと入力カメラ付近にfloater（ホコリみたいなやつ）が積層していくので、N=3000イテレーションごとに$\\alpha$を0に近づける。 他にも大きすぎるガウシアンは排除する。\n高速な微分可能なガウシアンのラスタライズ 目標＝高速なレンダリング＋高速なソート\n$\\rightarrow$ $\\alpha$ブレンディングをやったり、スプラットの回数制限を避けるため\n$\\rightarrow$タイルベースのラスタライズ＝プリミティブを画像全体に対して事前にソートして、ピクセルごとにソートを行う計算量を避ける ref\nこのラスタライズは任意の数の混合ガウシアンに対して効率的なバックプロパゲーションを行え、メモリ消費量が少ない。（ピクセルごとのオーバーヘッドが必要なだけ）\n画面を$16 \\times 16$のタイルに分割して、ビューの台形（frustum）に応じて3Dガウシアンをタイルごとに間引く。信頼区間99%のガウシアンだけ残す。 さらに、ガードバンドを設けてFrustumから近すぎたり、遠すぎたりするものを棄却する。（投影される二次元共分散の計算が不安定になりがちだから） タイルの重なりに応じてガウシアンをインスタンス化、その後深度とタイルIDを組み合わせてインスタンスにキーを割り当てる。 ガウシアンをキーに応じてソート（single fast GPU Radix sortを使う） このソートに基づいて$\\alpha$ブレンディングが行われる。\nはじめの方は$\\alpha$ブレンディングはいくつかの構成の近似として動作するが、スプラットがピクセルサイズに近づくにつれてこの近似は無視できるようになっていく。\nタイルにスプラットされ、深度でソートされたリストを生成し、タイルごとにラスタライズするためのスレッドブロックを起動しておく。各スレッドブロックは共有メモリにガウシアンパケットをロードして、各ピクセルに色と$\\alpha$をfront-to-back方向に累積していく。$\\alpha$値が飽和したらそのスレッドを停止させる。定期的にタイルのスレッドをチェックして、全ピクセルが飽和していたらスレッド全てを停止する。\n先行研究と異なり、勾配を受け取るプリミティブの数を制限してない。（シーン固有のハイパーパラメータが不要という意味）\n結果 Mildenhallの研究で使われた13つのリアルなシーンで実験。\nインドア・アウトドアデータで実験。Mip-NeRF360は品質、InstantNGPとPlenoxelsは速さ比較用途。8枚目をテスト用として使用。 定量評価 イテレーション数の違い Mip-NeRF360は48hかかるが、3D Gaussian Splattingは35-45minで学習できる。 InstantNGPやPlenoxelsは5-10minで終わるが、品質は劣る。\n合成データ(Blender)での結果。正確なカメラパラメータが得られる。ランダムな初期値でもかなり良い結果になる。 Ablations SfMの初期値を使うことの重要性。ランダムでも全体的には良い結果が得られるが、背景部分にはモヤ（floaters）が出現する。 ガウシアンの分割やクローンをやらない場合などの違い。背景を良くするには大きなガウシアンの分割が必要、一方小さなガウシアンのクローンは高品質で高速な収束に必要。 3Dガウシアンが異方性（楕円体）を持つ必要性の確認。等方性である（球体）だと明らかに低品質 Limitations 観測シーンが少ない場合はアーチファクトが発生 細長いアーチファクトやポツポツとした3Dガウシアンを作成する可能性がある 大きなガウシアンが作られた場合にポッピングアーチファクト（LODの切り替えタイミングで起こるアーチファクト）が時々発生 ポッピングのイメージ 視点位置依存のアピアランスの影響で発生するのでは ラスタライザのガードバンドにより棄却されてしまうのでは 単純な実装だと20GBくらい使うが、低レベル実装したら学習に数百MB、ラスタライズには追加で30-500MBくらいで済む（解像度依存）。 ","permalink":"https://blog.takumi-iida.com/posts/2023-11-14-3d-gaussian-splatting/","summary":"@Article{kerbl3Dgaussians, author = {Kerbl, Bernhard and Kopanas, Georgios and Leimkhler, Thomas and Drettakis, George}, title = {3D Gaussian Splatting for Real-Time Radiance Field Rendering}, journal = {ACM Transactions on Graphics}, number = {4}, volume = {42}, month = {July}, year = {2023}, url = {https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/} } Project Code 高解像度(1080p)で学習時間が短く、描画","title":"3D Gaussian Splatting"},{"content":" @InProceedings{pmlr-v202-salman23a, title = {Raising the Cost of Malicious AI-Powered Image Editing}, author = {Salman, Hadi and Khaddaj, Alaa and Leclerc, Guillaume and Ilyas, Andrew and Madry, Aleksander}, booktitle = {Proceedings of the 40th International Conference on Machine Learning}, pages = {29894--29918}, year = {2023}, editor = {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan}, volume = {202}, series = {Proceedings of Machine Learning Research}, month = {Jul}, publisher = {PMLR}, pdf = {https://proceedings.mlr.press/v202/salman23a/salman23a.pdf}, url = {https://proceedings.mlr.press/v202/salman23a.html}, abstract = {We present an approach to mitigating the risks of malicious image editing posed by large diffusion models. The key idea is to immunize images so as to make them resistant to manipulation by these models. This immunization relies on injection of imperceptible adversarial perturbations designed to disrupt the operation of the targeted diffusion models, forcing them to generate unrealistic images. We provide two methods for crafting such perturbations, and then demonstrate their efficacy. Finally, we discuss a policy component necessary to make our approach fully effective and practical—one that involves the organizations developing diffusion models, rather than individual users, to implement (and support) the immunization process.} } Project\nPaper\nCode\n悪意あるユーザによる画像編集を防ぐPhotoGuardを提案。フェイクニュースを防ぐことが目的。\n摂動を加えて、AIによる画像編集コストを上げる。（この論文では予防接種と呼んでいる）\nPhotoGuard 脅威モデルと解決策 攻撃方法（予防方法）は次の2種類。\nEncoder Attack Diffusion Attack Encoder Attack LDMのImage Encoder $\\mathbf{\\epsilon}$ に対してPGD攻撃する。\n\u0026ndash;\u0026gt; 悪いRepresentationを生成する。\nアーティファクトは発生しているが、目的の男性は生成できてしまっている。\nテキストを無視できていない。\n中間表現を攻撃しているだけ。生成結果の保証がない。\n\u0026ndash;\u0026gt; Diffusion Attack\nDiffusion Attack 最終的に生成された画像が失敗するようなPGD攻撃を行う。\n結果 対象モデル＝Stable Diffusion v1.5 目標＝無関係な画像の生成 or 非写実的な画像の生成\nEncoder Attackした結果。対策画像は非写実的な画像になっている。 画像編集で実験した場合の結果。「二人の頭部」は編集しないようなバイナリマスクが与えられている。 60の異なるテキストで編集した場合の結果。（今回は攻撃なので）値が悪いほうが良い。元画像の類似性。 プロンプトとの類似性 ","permalink":"https://blog.takumi-iida.com/posts/2023-10-01-photoguard/","summary":"@InProceedings{pmlr-v202-salman23a, title = {Raising the Cost of Malicious AI-Powered Image Editing}, author = {Salman, Hadi and Khaddaj, Alaa and Leclerc, Guillaume and Ilyas, Andrew and Madry, Aleksander}, booktitle = {Proceedings of the 40th International Conference on Machine Learning}, pages = {29894--29918}, year = {2023}, editor = {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan}, volume = {202}, series = {Proceedings of","title":"PhotoGuard"},{"content":" @misc{liang2023adversarial, title={Adversarial Example Does Good: Preventing Painting Imitation from Diffusion Models via Adversarial Examples}, author={Chumeng Liang and Xiaoyu Wu and Yang Hua and Jiaru Zhang and Yiming Xue and Tao Song and Zhengui Xue and Ruhui Ma and Haibing Guan}, year={2023}, eprint={2302.04578}, archivePrefix={arXiv}, primaryClass={cs.CV} } Project\nPaper\nCode\nVideo\nモンテカルロ法を用いて生成過程の潜在変数をおかしくするAEsを生成する手法を提案。PhotoGuardは実写画像がターゲットだったが、AdvDMはアートがターゲット。\n課題 AEsで拡散モデルを攻撃しようにも、分類モデルと比べて難しい 最適化のフローが変分境界を通して間接的に行われる=AEsが直接適用不可能 拡散モデルに対するAEsの既存の方法がない AdvDM コンセプト図。(1) 特徴抽出をOODにミスリード。(2) 摂動を付加して悪品質になるように最適化。 特徴抽出をOODにするノイズが生成されるように学習。その損失関数にモンテカルロ法を使う。\n前提 実分布を $ q(x) $ 生成分布を $ p(x) $ すると摂動 $ \\delta $ は次式で求められる。\nしかし、 $ q(x) $ は未知なので、モンテカルロ法を使って近似する。$ p_\\theta (x) $を使って、$ p_\\theta (x+\\delta) $を近似する。\n各時点の実分布の事後分布 $ q\\left(x_{1: T}^{\\prime} \\mid x_0^{\\prime}\\right) $は画像 $ x_0 $と独立な固定パラメータのガウス分布なので、生成分布 $ p_\\theta (x\\prime_(0:T)) $ は$ q\\left(x_{1: T}^{\\prime} \\mid x_0^{\\prime}\\right) $で正則化できる。\n最適化 $E_{x_{1: T} \\sim u\\left(x_{1: T}\\right)} \\mathcal{L}_{D M}(\\theta)$は期待値の損失なので、普通のAEsと違い勾配がわからない。そこで、モンテカルロ法を使って勾配を推定する。\n敵対的な生成分布 $ u (x\\prime_(1:T)) $ から $ x\\prime_(1:T) $ をサンプリングして、$ L_{D M}(\\theta) $ の勾配を推定する。\nこの推定された勾配を使ってFGSMを行う。\n異なる潜在変数になる各サンプルをイテレーション。\nAdvDMの最適化（生成）フロー 評価 Note 入力画像を使わずに、完全なガウスノイズから生成した画像は評価対象外\n＝コピーライトの心配なし 特徴抽出された特徴がOODになっていることを評価 画像から実際に抽出される条件 $c_g$のほうが無条件でサンプリングされる $c$よりも画像との類似性が高いはず\nAdvの条件cがOODになっていることの評価。$D$にはFIDやPrecision(prec., Kynkaanniemi, 2019) が利用される CFGっぽい\nアートトレースが危惧されるシナリオ Text InversionベースのT2I Text Inversionベースのスタイル変換 I2Iの変換 結果 Text-InversionベースのT2I クリーン画像とAEsでスタイル変換したときの結果。（上段）クリーン画像（下段）AdvDMで作ったAEs。注：Strengthはスタイル変換の強さパラメータ。 スタイルを強く転送しようとすると、崩壊している感じがする。\n異なるサンプリングステップを変えてAdvDMした結果。ステップを増やすほどクオリティが低いものが生成できる。 加える摂動の大きさによる攻撃効果。 条件付きT2I AEsに対する防御策をAdvDMで試した結果。一定の効果がある。 次に読む論文 Mist: Towards Improved Adversarial Examples for Diffusion Models ","permalink":"https://blog.takumi-iida.com/posts/2023-09-27-advdm/","summary":"@misc{liang2023adversarial, title={Adversarial Example Does Good: Preventing Painting Imitation from Diffusion Models via Adversarial Examples}, author={Chumeng Liang and Xiaoyu Wu and Yang Hua and Jiaru Zhang and Yiming Xue and Tao Song and Zhengui Xue and Ruhui Ma and Haibing Guan}, year={2023}, eprint={2302.04578}, archivePrefix={arXiv}, primaryClass={cs.CV} } Project Paper Code Video モンテカルロ法を用いて生成過程の潜在変数を","title":"AdvDM"},{"content":" @inproceedings {291164, author = {Shawn Shan and Jenna Cryan and Emily Wenger and Haitao Zheng and Rana Hanocka and Ben Y. Zhao}, title = {Glaze: Protecting Artists from Style Mimicry by Text-to-Image Models}, booktitle = {32nd USENIX Security Symposium (USENIX Security 23)}, year = {2023}, isbn = {978-1-939133-37-3}, address = {Anaheim, CA}, pages = {2187--2204}, url = {https://www.usenix.org/conference/usenixsecurity23/presentation/shan}, publisher = {USENIX Association}, month = {Aug} } Project\nPaper\nアーティストのスタイルを模倣するテキストから画像への変換モデルの攻撃を防ぐツールGlazeを提案した。コンテンツを維持しつつ、スタイルは別物になるような摂動を加える。\n（Victimではない）ターゲットのスタイルに近づく特徴量シフトを生じさせ、スタイル学習に失敗するような摂動（\u0026ldquo;Style Cloak\u0026rdquo;）を加えることで、攻撃を防ぐ。 アーティストコミュニティと連携して、1156人の参加者を使って定性的な評価を行った。 93%の誤生成率、92%は元の画像スタイルを維持。 この論文では、手法の他にもアーティストが生成画像に対してどう思っているのかを記事を参照しながら紹介し、実際にトレースされた事例も挙げている。\nThreat Model アーティスト\nモデルに模倣されることを防ぎつつ作品を共有したい 知覚不能な摂動を共有前に自分の作品に加えて自己防衛したい ノーパソみたいな貧弱な計算資源しかないかも 模倣者（攻撃者）\nVictim Styleの高品質な画像を生成したい 重みにアクセス可能 ターゲットのアーティストの画像を数枚入手可能 十分な計算資源を持っている 攻撃シナリオ Glazeは学習されても模倣されないような画像を生成することを目指す。\n模倣防衛シナリオ 先行研究 顔画像保護 \u0026ldquo;image cloaking\u0026rdquo;: ユーザ画像の特徴表現から劇的に変わるような摂動を加える ただし、text-to-imageのような大規模な特徴空間では機能しない 画像生成するには多くの属性情報が特徴量に含まれるため、その空間上で同様の摂動を作るのは難しい（生成モデルに対する摂動が難しいという研究がある） PhotoGuard 生成モデルに対するAEsを行い許可のない画像編集を防ぐ方法 画像の情報をすべて最小化してしまうため、模倣は防げない（模倣は編集ではなく、FTによる学習が行われるため？） Glaze Glazeの全体像 Style Transferを使って、Victimのオリジナル画像を様々なスタイルに変換\nStyle-Transferの結果 シフト先のスタイル（ターゲット）の画像は、各アーティストとVictimの特徴中心を特徴抽出気（$\\Phi$）を使って計算。距離が50-75パーセンタイルにあるものをターゲットスタイルとして候補に選択する。 このスタイル変換した画像を使って摂動をガイドする\n$$ \\begin{gathered} \\min _{\\delta_x} \\operatorname{Dist}\\left(\\Phi\\left(x+\\delta_x\\right), \\Phi(\\Omega(x, T))\\right) \\ \\text { subject to }\\left|\\delta_x\\right|\u0026lt;p \\end{gathered} $$ ここで、$\\Omega (x, T)$は画像xを既存のスタイル変換モデル$\\Omega$でTのスタイルに変換した画像、$\\delta_x$は摂動を表す。 最終的な損失関数は以下のようになる。 $$ \\min _{\\delta_x}\\left|\\Phi(\\Omega(x, T)), \\Phi\\left(x+\\delta_x\\right)\\right|_2^2+\\alpha \\cdot \\max \\left(\\operatorname{LPIPS}\\left(\\delta_x\\right)-p, 0\\right) $$\n結果 アーティストとCLIP Scoreによる防御率評価 定性評価 摂動量による防御率変化 摂動量の違いによる各指標 どれくらい摂動が加わっていたら投稿したいか （左）Victimと模倣者で別々の特徴抽出器$\\Phi$を使った場合。（右）すでにネット上に画像をアップロードしているアーティストの場合、全体の25%くらいしの作品を保護できていれば87.2%くらいの全体防御率になる。 ガウスノイズを加えてプロテクトを解除しようとしてもあまり効果がない。JPEG圧縮も同様。 補足 現実世界でのトレース事例 Hollie Mengertの作品がReddit上のモデルでトレースされた。 BAIO, A. Invasive Diffusion: How one unwilling illustrator found herself turned into an AI model, 2022\n漫画家Sarah Andersenが自身の作品がトレースできると報告 The Alt-Right Manipulated My Comic. Then A.I. Claimed It. The New York Times, 2022 その関連 MURPHY, B. P. Is Lensa AI Stealing From Human Art? An Expert Explains The Controversy. ScienceAlert, 2022. YANG, S. Why Artists are Fed Up with AI Art. Fayden Art, Dec. 2022. いくつかの会社はそれをサービスとして展開 SCENARIO.GG. AI-generated game assets, 2022 数枚の画像をアップロードすると、それに似たスタイルの画像を生成してくれる。 CivitAIはトレースした作品/モデルをシェアするプラットフォーム 感想 スタイルだけでOKなのか。コンテンツはトレースされそう。 AEsの傾向はそのまま維持されてそう ","permalink":"https://blog.takumi-iida.com/posts/2023-09-23-glaze/","summary":"@inproceedings {291164, author = {Shawn Shan and Jenna Cryan and Emily Wenger and Haitao Zheng and Rana Hanocka and Ben Y. Zhao}, title = {Glaze: Protecting Artists from Style Mimicry by Text-to-Image Models}, booktitle = {32nd USENIX Security Symposium (USENIX Security 23)}, year = {2023}, isbn = {978-1-939133-37-3}, address = {Anaheim, CA}, pages = {2187--2204}, url = {https://www.usenix.org/conference/usenixsecurity23/presentation/shan}, publisher = {USENIX Association}, month = {Aug} } Project Paper アー","title":"Glaze"},{"content":"動機とか 以前はJekyllを使って自前サイトを作っていたのですが、なんか微妙に感じていました。\nHugoもちょっと触ってみたいと思ったので、触ってみたら結構簡単でかつ好みのテーマがあったので、乗り換えてみようかなと感じました。\nもう見なくなるサイトなので、写真だけ貼って供養します。\n論文を時系列に並べて関係性を広い意味で整理しようと Archive を投稿順に並べるところに当時はこだわりましたが、あんまり使わなかったですね。 あと、あんまり長い記事は書かなかったので、記事のスクロールに合わせて「アウトライン」のハイライトが遷移する機能も必要なかったですね\u0026hellip;。\n旧ページのギャラリー トップページ\n記事ページ アーカイブページ\nわりかし苦労したやつ。レイアウトにこだわった。改善余地はあるが、当時の自分としては満足した。\n移行に当たってのメモ 環境 Git Go Hugo-extended\nバイナリファイルをダウンロード PaperMod hugo-embed-pdf Hugo Icons Module 起動コマンド忘れそうなのでメモ。\n# 起動 $ hugo server -D Custom ShortCode お知らせブロック Info A notice disclaimer Note A notice disclaimer Tip A notice disclaimer Warning A notice disclaimer BibTexの引用 @misc{liang2023adversarial, title={Adversarial Example Does Good: Preventing Painting Imitation from Diffusion Models via Adversarial Examples}, author={Chumeng Liang and Xiaoyu Wu and Yang Hua and Jiaru Zhang and Yiming Xue and Tao Song and Zhengui Xue and Ruhui Ma and Haibing Guan}, year={2023}, eprint={2302.04578}, archivePrefix={arXiv}, primaryClass={cs.CV} } いろんなリンク Project\nPaper\nCode\nVideo\nキャプション付きで画像をタイル状に並べる 画像1 画像2 画像3 テーマのカスタマイズ ホーム Biography: layouts/biography/single.html 記事一覧: layouts/_default/list.html ソート: laytout/_default/sorted-by-pubdate.html 画像添付: layouts/_default/_markup/render-link.html [ココ](link)がキャプションになるようにした 数式レンダラ: layouts/partial/math.html BibTexの引用: layouts/partial/cite.html, static/js/cite.js 出版年月を数値表記にするためJSを使用 ","permalink":"https://blog.takumi-iida.com/posts/2023-09-18-introduction/","summary":"動機とか 以前はJekyllを使って自前サイトを作っていたのですが、なんか微妙に感じていました。 Hugoもちょっと触ってみたいと思ったので、触","title":"移行しました"},{"content":" @InProceedings{Erhan_2014_CVPR, author = {Erhan, Dumitru and Szegedy, Christian and Toshev, Alexander and Anguelov, Dragomir}, title = {Scalable Object Detection using Deep Neural Networks}, booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, month = {June}, year = {2014} } Paper\nVideo\nMultiBoxは、画像中に複数インスタンスが会った場合でも、Confidence付きのbboxを複数出力可能なモデルです。これも重要論文に数えられるんですが、解説がなかなかありません。おそらく単純な方法だからだとおもうんですが、一応読んでみました。軽くさらっていきます。 概要 一枚の画像中で複数の同一インスタンスがあった場合に、複数のbboxをconfidence付きで出力できるようにした論文です。\n従来手法だと、一枚画像を入力して、各クラスに対して単一のbboxを推論するネットワークしかなかったらしいんですが、MultiBoxでは、画像中に複数の同一インスタンスがあっても、一発で複数の領域提案が行えるようにしています。 Multiboxはクラスに依らない領域提案を学習しているので、高い汎化能力があるそうです。\nMultibox 論文を読んだ感じだと、次図のアーキテクチャになると思います。共通のAlexNetのバックボーンがあり、そこからbbox回帰と物体らしさ(Objectness)のconfidenceを出力する2つのブランチがあります。\nMultiboxは、事前に一枚の画像に含まれる最大の物体数Kを決めておきます。そして、K個のbboxの位置とconfidenceを推定した後に、NMSやConfidenceを使って、不要なbboxの廃棄(supression)を行います。論文中では\\( K = 100, 200 \\)を使用しています。\nほとんど解説が終わってしまったんですが、一応各ブランチでの損失関数などを解説します。\nbbox回帰ブランチ bbox回帰ブランチでは、K個あるbbox候補のGT座標への回帰を行います。出力は、GT bboxの左上と右下のxy座標の4点です。\n損失関数は次式です。ここで、\\( x_{ij} \\)は、推論bbox\\( i \\)がGT bbox \\( j \\)と対応しているとき1になりそれ以外は0になるバイナリ値です。それ以外は普通にL2ロスで最適化しています。筆者は、この\\( x_{ij} \\)の理解が怪しいです。\n$$ F_{\\text {match }}(x, l)=\\frac{1}{2} \\sum_{i, j} x_{i j}\\left|l_{i}-g_{j}\\right|_{2}^{2} $$\nConfidence推定ブランチ Confidenceブランチでは、各bboxに対応したConfidence値を推定します。損失関数は、次式です。bbox回帰ブランチに入ってくる\\( x_{ij} \\)を除くと、バイナリクロスエントロピーロスと同じ形をしています。物体かどうかの判定をしています。推論bbox\\( i \\)のGT bbox \\( j \\)へのConfidenceが上昇すると損失が下がる構造になっています。\n$$ F_{\\mathrm{conf}}(x, c)=-\\sum_{i, j} x_{i j} \\log \\left(c_{i}\\right)-\\sum_{i}\\left(1-\\sum_{j} x_{i j}\\right) \\log \\left(1-c_{i}\\right) $$\n最後に上記２つのブランチの損失をマージした損失関数（次式）をまとめて最適化していきます。詳しい学習方法は割愛します。\n$$ F(x, l, c)=\\alpha F_{\\text {match }}(x, l)+F_{\\text {conf }}(x, c) $$\n","permalink":"https://blog.takumi-iida.com/posts/2021-07-25-multibox/","summary":"@InProceedings{Erhan_2014_CVPR, author = {Erhan, Dumitru and Szegedy, Christian and Toshev, Alexander and Anguelov, Dragomir}, title = {Scalable Object Detection using Deep Neural Networks}, booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, month = {June}, year = {2014} } Paper Video MultiBoxは、画像中に複数インスタンスが会","title":"MultiBox"},{"content":" @misc{sermanet2014overfeat, title={OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks}, author={Pierre Sermanet and David Eigen and Xiang Zhang and Michael Mathieu and Rob Fergus and Yann LeCun}, year={2014}, eprint={1312.6229}, archivePrefix={arXiv}, primaryClass={cs.CV} } Paper\nCode\nVideo\n従来手法とCNNを組み合わせた初の論文です。R-CNNと似たような位置づけだが、R-CNNは2ステージ検出器、OverFeatは1ステージ検出器の元祖といえると思います。\nOverFeatは、CNN for Object Detection最初期の論文ですが、R-CNNが話題に出ることが多く、重要な位置づけの論文の割に日本語の解説ページがあまりないことに気づきました。ほとんど英語の解説記事や解説動画の焼き増し的な記事になると思いますが、日本語解説を用意することに意義があると思いました。\n概要 OverFeatは、物体検出タスクを物体認識タスクと位置推定タスクを統合して実現した、スライディングウィンドウ方式の手法です。\n位置推定タスクと物体検出タスクの区別がしっくりと来ませんが、\n位置推定タスクは、画像中に何か物体があることは保証されていて、どこにその物体があるのかを推定するタスク 物体検出タスクは、画像中に物体があることは保証されておらず、何もなければ結果を出力しないような処理が入るもの だと理解しました。\nこの論文では、これらを別々のタスクとしてまずは扱い、それらを統合するようなフレームワークになっています。\nOverFeatのフレームワーク 論文本来のフローとかなり流れが違いますが、どうにもわかりにくいので、Cogneethiの動画をベースに解説していきます。もち違っている点があれば、ツイッターとかでご指摘ください。\nOverFeatのクラス分類・位置推定のブランチは、画像全体のCNNの特徴マップを直接受け取って、最後にそれら２つを統合する処理が入っています。\n全体のざっくりした処理フローは、次の図のようになっており、いわゆる領域提案は行わず、1ステージの構造になっています。CNNの部分はAlexNetベースのネットワークになっており、ImageNetで事前に学習済みのものを利用します。\nそれぞれのモジュールについて解説していきます。\n分類ブランチ 分類ブランチの中身はFully Convolutional Network(FCN)で構成されており、Conv層のみです。そのため、任意のサイズの入力を受け取ることができます。\nOverFeatは、複数スケールの入力を受け取り、異なるサイズの特徴マップを出力します。すると、Conv層には、受容野があるので、次図の5x5の畳み込みが分類ブランチの一回目で行われれば、特徴の由来の対応関係がグレーのセルのようになります。\nすると、この特徴マップのセルの中に物体があるかどうかを判定できれば、入力画像で粗いスライディングウィンドウをやったのと同じような扱いにすることができます。つまり、スライディングウィンドウのように実際に入力画像を切り出すのではなく、画像全体を入力して、特徴空間で切り出しをしているようなイメージです。空間的な探索空間が大幅に減るので、かなり高速化されます。281x317サイズで画像を入力したときの、2x3の出力で左上のセルに物体があると判定されれば、画像の左上の方にも物体があると解釈することができます。\n回帰ブランチ 分類ブランチで、大まかにどこあたりに物体がありそうかというのはわかりますが、かなり解像度があらいので、入力画像上でどの位置にあるのかを求める必要があり、回帰ブランチではそれを行います。\n回帰ブランチの構造は、分類ブランチとほぼ同様です。違うのは、最後の出力サイズくらいです。bboxの座標を表現する4点(x1, y1, x2, y2)を推論するために、マップが4倍になっています。\n統合 最後に、分類ブランチと回帰ブランチの出力結果の統合を行います。\n現在よく行われているNMSではなく、貪欲な方法で統合を行っています。\n統合の手順は以下の流れです。\n各スケール\\(s \\in 1\u0026hellip;6 \\)でtop-kのクラス集合を\\(C_s \\)に割当てる。top-kは、各スケールの空間位置を跨いで、計算します。 スケール\\(s \\)で、全ての空間位置に対して、クラス集合\\(C_s \\)に対応するクラスのbbox出力を\\(B_s \\)に割り当てる。 各スケールのbbox集合\\(B_s \\)を一つの集合にする。(\\(B \\leftarrow \\bigcup_{s} B_{s} \\)) 各bbox候補をマージしていく。bbox同士のマッチスコアが小さい順からマージして、しきい値を超えると、マージを止める。 4. のbbox同士のマッチスコアは、bboxの中心の座標同士の距離を全て計算します。計算した結果の昇順の組み合わせで、マージを実行していきます。マージ後のbboxは、それぞれのbboxの平均になります。\n感想 言わずもがな、1ステージ検出器の開拓手法として非常に重要な論文ですね。 複数入力必要なのが難点ですが、フォワード計算はかなり高速なのが嬉しい。 一方で、検出性能がR-CNNよりも劣るのが難点。 まあ、スキップコネクションなしで、特徴マップ上でのObjectnessを計算しなきゃいけないのと、 分類ブランチの学習がマルチスケールに対応できていないような気がするので、推論時にギャップが生じている気がする。\n参考 C 5.6 Overfeat , Network Design Important-Dont skip, CNN, Object Detection , EvODN OverFeat , Lecture 38 (Part 1) , Applied Deep Learning Overfeat Review(1312.6229) ","permalink":"https://blog.takumi-iida.com/posts/2021-07-24-overfeat/","summary":"@misc{sermanet2014overfeat, title={OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks}, author={Pierre Sermanet and David Eigen and Xiang Zhang and Michael Mathieu and Rob Fergus and Yann LeCun}, year={2014}, eprint={1312.6229}, archivePrefix={arXiv}, primaryClass={cs.CV} } Paper Code Video 従来手法とCNNを組み合わせた初の論文です。R-CNNと似たような位置","title":"OverFeat"},{"content":" @InProceedings{Girshick_2014_CVPR, author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra}, title = {Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation}, booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, month = {June}, year = {2014} } Paper\nCode\nCNNと領域提案手法を統合した先駆け的論文。従来のSoTAから大きく性能が向上したが、一方で多段階のパイプラインに難ありで、実行時間も長いのが難点。\n概要 R-CNNは、CNNを物体検出に取り込んだ先駆け論文として広く知られているモデルです。 従来の方法では、スライディングウィンドウで、超大な数の領域をあらかた提案してCNNに突っ込んで学習するような方法が考えられてきました。しかし、それだと探索範囲が広すぎるため、より効率的に物体らしい領域を提案する必要がありました。R-CNNはSelective Searchを利用して、効率的に領域提案を行い、CNNが実行可能な程度に効率化を図ったモデルです。\nR-CNNの実行手順は以下の流れになります。\nSelective Searchを使った領域提案 で提案された領域を入力画像から切り出し 切り出した領域を一定サイズにワープ ワープした画像をCNNに入力 複数のSVMでクラス分類、bbox回帰を行う。 もう少し詳しく解説します。\nSelective Searchによる領域提案 まず、1. でR-CNNではSelective Searchを利用した領域提案を行っています。Selective Searchでは、次の手順で領域提案を行います。解説はかなりの部分をしこあんさんのブログをベースとしています。\nFelzenszwalb法による画像のセグメンテーションを行い、各セグメントを一つの領域候補とする。 各領域候補の色とテクスチャのヒストグラム特徴量を作成する。 各領域同士で重なりのあるセグメントの組み合わせをNeighborとして全列挙する。 の特徴量を使い、3. の全ての組み合わせから領域候補の類似度を計算。類似度計算には、下記の4項目を利用する。 色（2. で計算済みのRGBのヒストグラム特徴量） テクスチャ(2. で計算済みのLocal Binary Pattern(LBP)のヒストグラム特徴量) サイズ オーバーラップ度合い での類似度が高いもの同士をマージする。（Hierarical Search） LBPは、エッジの情報がロバストに抽出できる手法として知られています。グレースケール化後、注目ピクセルの周囲に対して自身よりも値が、大きい（\u0026ndash;\u0026gt; 1）小さい（\u0026ndash;\u0026gt; 0）かのバイナリ値を作成した後に、真上から反時計回りに並べた2進数を出力します。\nこちらの記事より作成。\n試しにsk-imageの関数でLBPをした結果が次の図です。エッジが保たれ、テクスチャが抽出されていることが確認できます。\nこのLBP特徴画像とRGBの各チャネルに対して2. でヒストグラム特徴量を求めます。そして、隣接するセグメント同士でヒストグラム特徴量同士の類似度を計算して、再帰的に類似度が高いセグメントを結合していきます。セグメントと書いていますが、実際にはセグメントを囲む提案領域の矩形をマージしています。 これにより、Selective Searchによる領域提案を行います。\nテスト時には2000個程度の領域が提案されます。\n特徴抽出/クラス分類/ bbox回帰 Selective Searchによって提案された領域は元画像からクロップされて一定サイズにワープされます。論文中では、特徴抽出にはAlexNetが使われているので、227x227サイズのRGB画像にリサイズされて、4096次元の特徴ベクトルが各提案領域について出力されます。具体的には5つのConv層と2つのFC層を通過します。FC層が挟まっているのもあって、固定サイズにリサイズしなければいけません。\nこうして提案領域のCNN特徴量は、複数のSVMを使ってクラススコアを計算し、各クラスで独立にNMSさせて結果を出力します。SVMは一緒に学習するのではなく、各クラスで事前学習したものを利用します。\nbbox回帰では、線形回帰モデルを使っている。DPMという手法に影響されたらしい。\n感想 Faster R-CNNよりは需要低いけど、古典的アルゴからCNNベースへの橋渡し的な位置づけの論文だと思うので、抑えておくのは重要だと感じた。 複数SVMをクラス分類に利用していて、かつ事前学習しなければいけないのはかなり面倒だし計算コストも高いと感じた。 ","permalink":"https://blog.takumi-iida.com/posts/2021-07-24-r-cnn/","summary":"@InProceedings{Girshick_2014_CVPR, author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra}, title = {Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation}, booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, month = {June}, year = {2014} } Paper Code CNNと領域提案手法を統合した先駆け的論文","title":"R-CNN"},{"content":"","permalink":"https://blog.takumi-iida.com/pubdate/","summary":"","title":"記事で紹介した論文の出版年"},{"content":"","permalink":"https://blog.takumi-iida.com/aboutme/","summary":"","title":"飯田啄巳"}]