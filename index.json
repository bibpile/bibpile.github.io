[{"content":"動機とか 以前はJekyllを使って自前サイトを作っていたのですが、なんか微妙に感じていました。\nHugoもちょっと触ってみたいと思ったので、触ってみたら結構簡単でかつ好みのテーマがあったので、乗り換えてみようかなと感じました。\nもう見なくなるサイトなので、写真だけ貼って供養します。\n論文を時系列に並べて関係性を広い意味で整理しようと Archive を投稿順に並べるところに当時はこだわりましたが、あんまり使わなかったですね。 あと、あんまり長い記事は書かなかったので、記事のスクロールに合わせて「アウトライン」のハイライトが遷移する機能も必要なかったですね\u0026hellip;。\n旧ページのギャラリー トップページ\n記事ページ アーカイブページ\nわりかし苦労したやつ。レイアウトにこだわった。改善余地はあるが、当時の自分としては満足した。\n移行に当たってのメモ 環境 Git Go Hugo-extended\nバイナリファイルをダウンロード PaperMod hugo-embed-pdf Hugo Icons Module 起動コマンド忘れそうなのでメモ。\n# 起動 $ hugo server -D ","permalink":"https://blog.takumi-iida.com/posts/2023-09-18-introduction/","summary":"動機とか 以前はJekyllを使って自前サイトを作っていたのですが、なんか微妙に感じていました。 Hugoもちょっと触ってみたいと思ったので、触","title":"移行しました"},{"content":" @InProceedings{Erhan_2014_CVPR, author = {Erhan, Dumitru and Szegedy, Christian and Toshev, Alexander and Anguelov, Dragomir}, title = {Scalable Object Detection using Deep Neural Networks}, booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, month = {June}, year = {2014} } Paper\nVideo\nMultiBoxは、画像中に複数インスタンスが会った場合でも、Confidence付きのbboxを複数出力可能なモデルです。これも重要論文に数えられるんですが、解説がなかなかありません。おそらく単純な方法だからだとおもうんですが、一応読んでみました。軽くさらっていきます。 概要 一枚の画像中で複数の同一インスタンスがあった場合に、複数のbboxをconfidence付きで出力できるようにした論文です。\n従来手法だと、一枚画像を入力して、各クラスに対して単一のbboxを推論するネットワークしかなかったらしいんですが、MultiBoxでは、画像中に複数の同一インスタンスがあっても、一発で複数の領域提案が行えるようにしています。 Multiboxはクラスに依らない領域提案を学習しているので、高い汎化能力があるそうです。\nMultibox 論文を読んだ感じだと、次図のアーキテクチャになると思います。共通のAlexNetのバックボーンがあり、そこからbbox回帰と物体らしさ(Objectness)のconfidenceを出力する2つのブランチがあります。\nMultiboxは、事前に一枚の画像に含まれる最大の物体数Kを決めておきます。そして、K個のbboxの位置とconfidenceを推定した後に、NMSやConfidenceを使って、不要なbboxの廃棄(supression)を行います。論文中では\\( K = 100, 200 \\)を使用しています。\nほとんど解説が終わってしまったんですが、一応各ブランチでの損失関数などを解説します。\nbbox回帰ブランチ bbox回帰ブランチでは、K個あるbbox候補のGT座標への回帰を行います。出力は、GT bboxの左上と右下のxy座標の4点です。\n損失関数は次式です。ここで、\\( x_{ij} \\)は、推論bbox\\( i \\)がGT bbox \\( j \\)と対応しているとき1になりそれ以外は0になるバイナリ値です。それ以外は普通にL2ロスで最適化しています。筆者は、この\\( x_{ij} \\)の理解が怪しいです。\n$$ F_{\\text {match }}(x, l)=\\frac{1}{2} \\sum_{i, j} x_{i j}\\left|l_{i}-g_{j}\\right|_{2}^{2} $$\nConfidence推定ブランチ Confidenceブランチでは、各bboxに対応したConfidence値を推定します。損失関数は、次式です。bbox回帰ブランチに入ってくる\\( x_{ij} \\)を除くと、バイナリクロスエントロピーロスと同じ形をしています。物体かどうかの判定をしています。推論bbox\\( i \\)のGT bbox \\( j \\)へのConfidenceが上昇すると損失が下がる構造になっています。\n$$ F_{\\mathrm{conf}}(x, c)=-\\sum_{i, j} x_{i j} \\log \\left(c_{i}\\right)-\\sum_{i}\\left(1-\\sum_{j} x_{i j}\\right) \\log \\left(1-c_{i}\\right) $$\n最後に上記２つのブランチの損失をマージした損失関数（次式）をまとめて最適化していきます。詳しい学習方法は割愛します。\n$$ F(x, l, c)=\\alpha F_{\\text {match }}(x, l)+F_{\\text {conf }}(x, c) $$\n","permalink":"https://blog.takumi-iida.com/posts/2021-07-25-multibox/","summary":"@InProceedings{Erhan_2014_CVPR, author = {Erhan, Dumitru and Szegedy, Christian and Toshev, Alexander and Anguelov, Dragomir}, title = {Scalable Object Detection using Deep Neural Networks}, booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, month = {June}, year = {2014} } Paper Video MultiBoxは、画像中に複数インスタンスが会","title":"(MultiBox) Scalable Object Detection using Deep Neural Networks, CVPR, 2014"},{"content":" @misc{sermanet2014overfeat, title={OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks}, author={Pierre Sermanet and David Eigen and Xiang Zhang and Michael Mathieu and Rob Fergus and Yann LeCun}, year={2014}, eprint={1312.6229}, archivePrefix={arXiv}, primaryClass={cs.CV} } Paper\nCode\nVideo\n従来手法とCNNを組み合わせた初の論文です。R-CNNと似たような位置づけだが、R-CNNは2ステージ検出器、OverFeatは1ステージ検出器の元祖といえると思います。\nOverFeatは、CNN for Object Detection最初期の論文ですが、R-CNNが話題に出ることが多く、重要な位置づけの論文の割に日本語の解説ページがあまりないことに気づきました。ほとんど英語の解説記事や解説動画の焼き増し的な記事になると思いますが、日本語解説を用意することに意義があると思いました。\n概要 OverFeatは、物体検出タスクを物体認識タスクと位置推定タスクを統合して実現した、スライディングウィンドウ方式の手法です。\n位置推定タスクと物体検出タスクの区別がしっくりと来ませんが、\n位置推定タスクは、画像中に何か物体があることは保証されていて、どこにその物体があるのかを推定するタスク 物体検出タスクは、画像中に物体があることは保証されておらず、何もなければ結果を出力しないような処理が入るもの だと理解しました。\nこの論文では、これらを別々のタスクとしてまずは扱い、それらを統合するようなフレームワークになっています。\nOverFeatのフレームワーク 論文本来のフローとかなり流れが違いますが、どうにもわかりにくいので、Cogneethiの動画をベースに解説していきます。もち違っている点があれば、ツイッターとかでご指摘ください。\nOverFeatのクラス分類・位置推定のブランチは、画像全体のCNNの特徴マップを直接受け取って、最後にそれら２つを統合する処理が入っています。\n全体のざっくりした処理フローは、次の図のようになっており、いわゆる領域提案は行わず、1ステージの構造になっています。CNNの部分はAlexNetベースのネットワークになっており、ImageNetで事前に学習済みのものを利用します。\nそれぞれのモジュールについて解説していきます。\n分類ブランチ 分類ブランチの中身はFully Convolutional Network(FCN)で構成されており、Conv層のみです。そのため、任意のサイズの入力を受け取ることができます。\nOverFeatは、複数スケールの入力を受け取り、異なるサイズの特徴マップを出力します。すると、Conv層には、受容野があるので、次図の5x5の畳み込みが分類ブランチの一回目で行われれば、特徴の由来の対応関係がグレーのセルのようになります。\nすると、この特徴マップのセルの中に物体があるかどうかを判定できれば、入力画像で粗いスライディングウィンドウをやったのと同じような扱いにすることができます。つまり、スライディングウィンドウのように実際に入力画像を切り出すのではなく、画像全体を入力して、特徴空間で切り出しをしているようなイメージです。空間的な探索空間が大幅に減るので、かなり高速化されます。281x317サイズで画像を入力したときの、2x3の出力で左上のセルに物体があると判定されれば、画像の左上の方にも物体があると解釈することができます。\n回帰ブランチ 分類ブランチで、大まかにどこあたりに物体がありそうかというのはわかりますが、かなり解像度があらいので、入力画像上でどの位置にあるのかを求める必要があり、回帰ブランチではそれを行います。\n回帰ブランチの構造は、分類ブランチとほぼ同様です。違うのは、最後の出力サイズくらいです。bboxの座標を表現する4点(x1, y1, x2, y2)を推論するために、マップが4倍になっています。\n統合 最後に、分類ブランチと回帰ブランチの出力結果の統合を行います。\n現在よく行われているNMSではなく、貪欲な方法で統合を行っています。\n統合の手順は以下の流れです。\n各スケール\\(s \\in 1\u0026hellip;6 \\)でtop-kのクラス集合を\\(C_s \\)に割当てる。top-kは、各スケールの空間位置を跨いで、計算します。 スケール\\(s \\)で、全ての空間位置に対して、クラス集合\\(C_s \\)に対応するクラスのbbox出力を\\(B_s \\)に割り当てる。 各スケールのbbox集合\\(B_s \\)を一つの集合にする。(\\(B \\leftarrow \\bigcup_{s} B_{s} \\)) 各bbox候補をマージしていく。bbox同士のマッチスコアが小さい順からマージして、しきい値を超えると、マージを止める。 4. のbbox同士のマッチスコアは、bboxの中心の座標同士の距離を全て計算します。計算した結果の昇順の組み合わせで、マージを実行していきます。マージ後のbboxは、それぞれのbboxの平均になります。\n感想 言わずもがな、1ステージ検出器の開拓手法として非常に重要な論文ですね。 複数入力必要なのが難点ですが、フォワード計算はかなり高速なのが嬉しい。 一方で、検出性能がR-CNNよりも劣るのが難点。 まあ、スキップコネクションなしで、特徴マップ上でのObjectnessを計算しなきゃいけないのと、 分類ブランチの学習がマルチスケールに対応できていないような気がするので、推論時にギャップが生じている気がする。\n参考 C 5.6 Overfeat , Network Design Important-Dont skip, CNN, Object Detection , EvODN OverFeat , Lecture 38 (Part 1) , Applied Deep Learning Overfeat Review(1312.6229) ","permalink":"https://blog.takumi-iida.com/posts/2021-07-24-overfeat/","summary":"@misc{sermanet2014overfeat, title={OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks}, author={Pierre Sermanet and David Eigen and Xiang Zhang and Michael Mathieu and Rob Fergus and Yann LeCun}, year={2014}, eprint={1312.6229}, archivePrefix={arXiv}, primaryClass={cs.CV} } Paper Code Video 従来手法とCNNを組み合わせた初の論文です。R-CNNと似たような位置","title":"(OverFeat) OverFeat： Integrated Recognition, Localization and Detection using Convolutional Networks, arXiv:2014"},{"content":" @InProceedings{Girshick_2014_CVPR, author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra}, title = {Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation}, booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, month = {June}, year = {2014} } Paper\nCode\nCNNと領域提案手法を統合した先駆け的論文。従来のSoTAから大きく性能が向上したが、一方で多段階のパイプラインに難ありで、実行時間も長いのが難点。\n概要 R-CNNは、CNNを物体検出に取り込んだ先駆け論文として広く知られているモデルです。 従来の方法では、スライディングウィンドウで、超大な数の領域をあらかた提案してCNNに突っ込んで学習するような方法が考えられてきました。しかし、それだと探索範囲が広すぎるため、より効率的に物体らしい領域を提案する必要がありました。R-CNNはSelective Searchを利用して、効率的に領域提案を行い、CNNが実行可能な程度に効率化を図ったモデルです。\nR-CNNの実行手順は以下の流れになります。\nSelective Searchを使った領域提案 で提案された領域を入力画像から切り出し 切り出した領域を一定サイズにワープ ワープした画像をCNNに入力 複数のSVMでクラス分類、bbox回帰を行う。 もう少し詳しく解説します。\nSelective Searchによる領域提案 まず、1. でR-CNNではSelective Searchを利用した領域提案を行っています。Selective Searchでは、次の手順で領域提案を行います。解説はかなりの部分をしこあんさんのブログをベースとしています。\nFelzenszwalb法による画像のセグメンテーションを行い、各セグメントを一つの領域候補とする。 各領域候補の色とテクスチャのヒストグラム特徴量を作成する。 各領域同士で重なりのあるセグメントの組み合わせをNeighborとして全列挙する。 の特徴量を使い、3. の全ての組み合わせから領域候補の類似度を計算。類似度計算には、下記の4項目を利用する。 色（2. で計算済みのRGBのヒストグラム特徴量） テクスチャ(2. で計算済みのLocal Binary Pattern(LBP)のヒストグラム特徴量) サイズ オーバーラップ度合い での類似度が高いもの同士をマージする。（Hierarical Search） LBPは、エッジの情報がロバストに抽出できる手法として知られています。グレースケール化後、注目ピクセルの周囲に対して自身よりも値が、大きい（\u0026ndash;\u0026gt; 1）小さい（\u0026ndash;\u0026gt; 0）かのバイナリ値を作成した後に、真上から反時計回りに並べた2進数を出力します。\nこちらの記事より作成。\n試しにsk-imageの関数でLBPをした結果が次の図です。エッジが保たれ、テクスチャが抽出されていることが確認できます。\nこのLBP特徴画像とRGBの各チャネルに対して2. でヒストグラム特徴量を求めます。そして、隣接するセグメント同士でヒストグラム特徴量同士の類似度を計算して、再帰的に類似度が高いセグメントを結合していきます。セグメントと書いていますが、実際にはセグメントを囲む提案領域の矩形をマージしています。 これにより、Selective Searchによる領域提案を行います。\nテスト時には2000個程度の領域が提案されます。\n特徴抽出/クラス分類/ bbox回帰 Selective Searchによって提案された領域は元画像からクロップされて一定サイズにワープされます。論文中では、特徴抽出にはAlexNetが使われているので、227x227サイズのRGB画像にリサイズされて、4096次元の特徴ベクトルが各提案領域について出力されます。具体的には5つのConv層と2つのFC層を通過します。FC層が挟まっているのもあって、固定サイズにリサイズしなければいけません。\nこうして提案領域のCNN特徴量は、複数のSVMを使ってクラススコアを計算し、各クラスで独立にNMSさせて結果を出力します。SVMは一緒に学習するのではなく、各クラスで事前学習したものを利用します。\nbbox回帰では、線形回帰モデルを使っている。DPMという手法に影響されたらしい。\n感想 Faster R-CNNよりは需要低いけど、古典的アルゴからCNNベースへの橋渡し的な位置づけの論文だと思うので、抑えておくのは重要だと感じた。 複数SVMをクラス分類に利用していて、かつ事前学習しなければいけないのはかなり面倒だし計算コストも高いと感じた。 ","permalink":"https://blog.takumi-iida.com/posts/2021-07-24-r-cnn/","summary":"@InProceedings{Girshick_2014_CVPR, author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra}, title = {Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation}, booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, month = {June}, year = {2014} } Paper Code CNNと領域提案手法を統合した先駆け的論文","title":"(R-CNN) Rich feature hierarchies for accurate object detection and semantic segmentation, CVPR, 2014"},{"content":"","permalink":"https://blog.takumi-iida.com/pubdate/","summary":"","title":"記事で紹介した論文の出版年"},{"content":"","permalink":"https://blog.takumi-iida.com/aboutme/","summary":"","title":"飯田啄巳"}]